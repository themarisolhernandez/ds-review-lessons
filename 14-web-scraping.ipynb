{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 - Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Web Scraping?\n",
    "**Web Scraping** is a technique used to automatically extract information from websites. It involves accessing the HTML content of a web page, parsing that content, and extracting the data that we're interested in. This can include collecting text, images, links, and structured data such as tables.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <figure align=\"center\">\n",
    "    <img src=\"imgs/web_scraping1.png\" alt=\"Alt text\">\n",
    "    <figcaption>Source: WebHarvey</figcaption>\n",
    "  </figure>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Web Scraping Useful?\n",
    "- **Automated Data Collection**: Often, the data we need is scattered across various web pages, and manually collecting this data would be inefficient and error-prone. Web scraping automates this process.\n",
    "\n",
    "- **Access to Unstructured Data**: Many websites display unstructured data that is not readily available through traditional APIs. Web scraping helps convert this unstructured data into a structured format, making it easier to analyze and use in various applications.\n",
    "\n",
    "- **Generating Datasets**: Machine learning models often require large datasets, and scraping can be an effective way to collect training data from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world applications of web scraping:\n",
    "- **Price Comparison**: Collecting product prices from multiple e-commerce platforms to compare prices and identify the best deals.\n",
    "\n",
    "- **Employment Trends Analysis**: Scraping job postings to analyze industry demand, skill requirements, and employment patterns over time.\n",
    "\n",
    "- **Sentiment Analysis**: Gathering content from news outlets or social media platforms to assess public sentiment and trends based on text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction to HTTP and HTML\n",
    "To scrape a website, it’s essential to understand two key concepts: **HTTP** and **HTML**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is HTTP?\n",
    "**HTTP (HyperText Transfer Protocol)** is the protocol used to transfer data over the web. \n",
    "\n",
    "It is a request-response protocol, meaning the client (your browser or Python code) sends a request to a web server, and the server responds with data.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <figure align=\"center\">\n",
    "    <img src=\"imgs/web_scraping2.webp\" alt=\"Alt text\" width=\"550\" height=\"250\">\n",
    "    <figcaption>Source: JC Chouinard</figcaption>\n",
    "  </figure>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How HTTP Works:\n",
    "- **Client-Server Communication**: The client sends an HTTP request, and the server sends back an HTTP response. This response could be HTML (which we will scrape), JSON, or another type of data.\n",
    "    \n",
    "    - **Example**: When you enter \"https://example.com\" into your browser, your browser sends a GET request to the server hosting that website. The server responds with the website’s HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Example: Making an HTTP request using the requests library\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://example.com\")\n",
    "print(response.text[:500])  # Output the first 500 characters of the HTML content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is HTML?\n",
    "**HTML (HyperText Markup Language)** is the language used to create and structure web pages. When you visit a website, the browser receives an HTML document from the server and renders it into a web page. It consists of a series of **tags** that define elements such as headings, paragraphs, links, tables, and more.\n",
    "\n",
    "HTML structure is hierarchical:\n",
    "- **Tags**: Tags are the building blocks of HTML (e.g., `<p>` for paragraphs, `<h1>` for headings).\n",
    "\n",
    "- **Attributes**: Tags can have attributes (e.g., `<a href=\"https://example.com\">Link</a>` where `href` is an attribute).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/web_scraping3.png\" alt=\"Alt text\"  width=\"550\" height=\"450\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why HTML is Important for Web Scraping:\n",
    "Since the content of most websites is written in HTML, we need to understand its structure to extract the information we need efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of HTTP Requests\n",
    "The type of HTTP request made depends on the action you want to perform. Here are the four most commonly used methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GET`\n",
    "- **Purpose**: Retrieves data from a server.\n",
    "\n",
    "- **Common Usage**: Loading a webpage, downloading a file, or requesting specific resources (like images or documents).\n",
    "\n",
    "- **How it works**: The client sends a `GET` request to a server, asking for data without sending any data itself.\n",
    "\n",
    "- **Example**: A `GET` request is made to the URL https://example.com. The server responds, and the status code `200` indicates that the request was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"https://example.com\")\n",
    "print(response.status_code)  # 200 indicates success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `POST`\n",
    "- **Purpose**: Sends data to a server (e.g., submitting a form).\n",
    "\n",
    "- **Common Usage**: Used when you need to upload data to a server or send information, like login credentials or form data.\n",
    "\n",
    "- **How it works**: The client sends data (like a form submission) along with the `POST` request.\n",
    "\n",
    "- **Example**: A `POST` request is sent to the URL https://example.com. The data being sent includes a username and password, which are submitted to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\"https://example.com\", data={\"username\": \"user\", \"password\": \"pass\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PUT`\n",
    "- **Purpose**: Updates a resource on the server.\n",
    "\n",
    "- **Common Usage**: Used for updating information that already exists on the server.\n",
    "\n",
    "- **How it works**: The client sends data along with the `PUT` request to replace or update a resource.\n",
    "\n",
    "- **Example**: The `PUT` request is used to update a resource (in this case, an item with ID `1`) with new data (a new name and email)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.put(\"https://example.com/update/1\", data={\"name\": \"New Name\", \"email\": \"newemail@example.com\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DELETE`\n",
    "- **Purpose**: Deletes a specified resource from the server.\n",
    "\n",
    "- **Common Usage**: Removing records or data from a server.\n",
    "\n",
    "- **How it works**: The client sends a `DELETE` request to remove specific data from the server.\n",
    "\n",
    "- **Example**: A `DELETE` request is sent to https://example.com/delete/1, where the resource with ID `1` is deleted from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.delete(\"https://example.com/delete/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP Response Codes:\n",
    "HTTP responses come with status codes that indicate the result of the request. Here are a few common status codes:\n",
    "\n",
    "- `200`: OK (the request was successful)\n",
    "\n",
    "- `404`: Not Found (the requested resource was not found on the server.)\n",
    "\n",
    "- `500`: Internal Server Error (the server encountered an error)\n",
    "\n",
    "- For a complete list of HTTP status codes, visit [this link](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request was successful\n"
     ]
    }
   ],
   "source": [
    "# Example of checking the status code\n",
    "response = requests.get(\"https://example.com\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Request was successful\")\n",
    "else:\n",
    "    print(f\"Failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding these request types will help in choosing the right method when scraping data or interacting with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up for Web Scraping: Installing Python Libraries\n",
    "To scrape web pages efficiently, we need two Python libraries:\n",
    "- `requests`: This library allows us to send HTTP requests to download web pages.\n",
    "\n",
    "- `BeautifulSoup`: This library allows us to parse HTML and extract specific content from it.\n",
    "\n",
    "### Installation:\n",
    "Before we begin, install these libraries using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, we can begin scraping static web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Web Page Content\n",
    "The first step in web scraping is to fetch the HTML content of the page we want to scrape. We use the `requests` library to make an HTTP request and retrieve the page’s HTML.\n",
    "\n",
    "### Steps to Fetch a Web Page:\n",
    "1. **Make a GET Request**: Send a `GET` request to the URL of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `requests` library was imported at the top of this notebook\n",
    "# import requests\n",
    "\n",
    "# Step 1: Fetch the HTML content of a webpage\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Check the Response Status**: Ensure that the request was successful (status code `200`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "else:\n",
    "    print(f\"Failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Extract the HTML**: Once the request is successful, extract the HTML content for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Quotes to Scrape</title>\n",
      "    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\n",
      "    <link rel=\"stylesheet\" href=\"/static/main.css\">\n",
      "    \n",
      "    \n",
      "</head>\n",
      "<body>\n",
      "    <div class=\"container\">\n",
      "        <div class=\"row header-box\">\n",
      "            <div class=\"col-md-8\">\n",
      "                <h1>\n",
      "                    <a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n",
      "                </h1>\n",
      "            </div>\n",
      "            <div cla\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Print the first 500 characters of the HTML content\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens Behind the Scenes:\n",
    "1. A `GET` request is sent to the server hosting https://quotes.toscrape.com/.\n",
    "\n",
    "2. The server responds with the HTML of the page.\n",
    "\n",
    "3. The HTML is stored in the `response.text` variable, and we can now parse it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML with BeautifulSoup\n",
    "Once the HTML content is fetched, the next step is to parse the document so we can extract specific data. \n",
    "\n",
    "**BeautifulSoup** provides an intuitive interface to navigate through the HTML tree structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How BeautifulSoup Works:\n",
    "- **Tree Structure**: BeautifulSoup treats the HTML document as a nested tree of elements. For example, an `<html>` tag contains a `<body>` tag, which may contain a series of `<div>` or `<p>` tags.\n",
    "\n",
    "- **Navigating the Tree**: We can navigate through this tree structure and extract the text or attributes of the elements we’re interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Quotes to Scrape\n",
      "  </title>\n",
      "  <link href=\"/static/bootstrap.min.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"/static/main.css\" rel=\"stylesheet\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <div class=\"container\">\n",
      "   <div class=\"row header-box\">\n",
      "    <div class=\"col-md-8\">\n",
      "     <h1>\n",
      "      <a href=\"/\" style=\"text-decoration: none\">\n",
      "       Quotes to Scrape\n",
      "      </a>\n",
      "     </h1>\n",
      "    </div>\n",
      "    <div class=\"col-md-4\">\n",
      "     <p>\n",
      "      <a href=\"/login\">\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 4: Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Pretty-print the first 500 characters of the parsed HTML\n",
    "print(soup.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.prettify()` function helps visualize the HTML content in a readable format, displaying the hierarchy of elements.\n",
    "\n",
    "### Common Methods in BeautifulSoup:\n",
    "- `.find()`: Finds the first matching element.\n",
    "\n",
    "- `.find_all()`: Finds all elements that match the criteria.\n",
    "\n",
    "- `.select()`: Selects elements using CSS selectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Specific Information from Web Pages\n",
    "After parsing the HTML, we can start extracting specific data, such as quotes, authors, or any other element that interests us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data by Tag Name:\n",
    "We can use the `.find()` or `.find_all()` methods to extract elements based on their tag names (like `<h1>`, `<p>`, etc.).\n",
    "\n",
    "#### Example: Extract all quotes from the page\n",
    "Let's say we want to extract all quotes from the page. After inspecting the HTML, we realize that all quotes exist in a `<span>` tag with `class='text'`. We can use `.find_all()` to extract all elements that match this criteria. This should extract all quotes from the page.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/web_scraping4.png\" alt=\"Alt text\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote 1: “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
      "Quote 2: “It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
      "Quote 3: “There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
      "Quote 4: “The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "Quote 5: “Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n"
     ]
    }
   ],
   "source": [
    "# Extract all quotes from the page\n",
    "quotes = soup.find_all('span', class_='text')\n",
    "\n",
    "# Display the first 5 quotes\n",
    "for i, quote in enumerate(quotes[:5]):\n",
    "    print(f\"Quote {i+1}: {quote.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Links\n",
    "Web pages often contain links that can be extracted using the `<a>` tag.\n",
    "\n",
    "#### Example: Extract all links from page\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/web_scraping5.png\" alt=\"Alt text\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "/login\n",
      "/author/Albert-Einstein\n",
      "/tag/change/page/1/\n",
      "/tag/deep-thoughts/page/1/\n"
     ]
    }
   ],
   "source": [
    "# Extract all links (anchor tags) from the page\n",
    "links = soup.find_all('a')\n",
    "links\n",
    "\n",
    "# Display the first 5 links\n",
    "for link in links[:5]:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get('href')` method extracts the URL of each link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CSS Selectors:\n",
    "CSS selectors provide a more flexible way to extract data by matching patterns in the HTML structure.\n",
    "\n",
    "#### Example:\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/web_scraping6.png\" alt=\"Alt text\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change\n",
      "deep-thoughts\n",
      "thinking\n",
      "world\n",
      "abilities\n"
     ]
    }
   ],
   "source": [
    "# Extracting elements using CSS selectors\n",
    "tags = soup.select('div.tags a.tag')\n",
    "for tag in tags[:5]:\n",
    "    print(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSS selectors allow you to target elements more precisely when tags alone are not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Storing Tables in Pandas\n",
    "Many websites display data in tables (e.g., product listings, stock prices). These tables can be extracted and stored in a Pandas DataFrame for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Extract a Table:\n",
    "For this example, we will use [the Wikipedia page on world population](https://en.wikipedia.org/wiki/World_population) and extract the \"Historical estimates of world population\" table.\n",
    "\n",
    "1. **Identify the Table**: First, navigate to the Wikipedia page on world population: https://en.wikipedia.org/wiki/World_population.\n",
    "\n",
    "    - On this page, there is a table with historical estimates of the world’s population, which we'll scrape.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/web_scraping7.png\" alt=\"Alt text\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Fetch the Web Page Content**: Use the `requests` library to fetch the HTML content of the Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n"
     ]
    }
   ],
   "source": [
    "# Wikipedia page with the table\n",
    "url = \"https://en.wikipedia.org/wiki/World_population\"\n",
    "\n",
    "# Send an HTTP request to fetch the page content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the response status\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch page: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Parse the HTML Content**: After fetching the page, use **BeautifulSoup** to parse the HTML and locate the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-toc-available\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   World population - Wikipedia\n",
      "  </title>\n",
      "  <script>\n",
      "   (function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-c\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Pretty-print the HTML to visualize the structure\n",
    "print(soup.prettify()[:1000])  # Show the first 1000 characters of the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Find and Extract the Table**: Now, we need to find the table on the page. This table is defined using the `<table>` tag. Let's locate the table and extract its rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tr>\n",
      "<th scope=\"row\">Population\n",
      "</th>\n",
      "<th scope=\"col\">1\n",
      "</th>\n",
      "<th scope=\"col\">2\n",
      "</th>\n",
      "<th scope=\"col\">3\n",
      "</th>\n",
      "<th scope=\"col\">4\n",
      "</th>\n",
      "<th scope=\"col\">5\n",
      "</th>\n",
      "<th scope=\"col\">6\n",
      "</th>\n",
      "<th scope=\"col\">7\n",
      "</th>\n",
      "<th scope=\"col\">8\n",
      "</th>\n",
      "<th scope=\"col\">9\n",
      "</th>\n",
      "<th scope=\"col\">10\n",
      "</th></tr>\n"
     ]
    }
   ],
   "source": [
    "# Find the table by its class name or id (using the class \"wikitable\" for Wikipedia tables)\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Extract all rows in the table\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Print the first row (header row) to inspect it\n",
    "print(rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Extract the Data from Each Row**: We need to loop through the table rows (`<tr>`) and extract the data from each cell (`<td>` and `<th>` tags for headers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Population', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], ['Year', '1804', '1927', '1960', '1974', '1987', '1999', '2011', '2022', '2037', '2057'], ['Years elapsed', '200,000+', '123', '33', '14', '13', '12', '12', '11', '15', '20']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the rows of data\n",
    "table_data = []\n",
    "\n",
    "# Loop through the rows in the table\n",
    "for row in rows:\n",
    "    # Extract the cells from the row\n",
    "    cells = row.find_all(['td', 'th'])\n",
    "    \n",
    "    # Extract the text from each cell and strip any surrounding whitespace\n",
    "    row_data = [cell.text.strip() for cell in cells]\n",
    "    \n",
    "    # Append the row data to our list\n",
    "    table_data.append(row_data)\n",
    "\n",
    "# Print the first few rows of extracted data\n",
    "print(table_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Convert the Data into a Pandas DataFrame**: Once we've extracted the table data, we can convert it into a Pandas `DataFrame`. This allows us to easily manipulate, analyze, or store the data for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Year</td>\n",
       "      <td>1804</td>\n",
       "      <td>1927</td>\n",
       "      <td>1960</td>\n",
       "      <td>1974</td>\n",
       "      <td>1987</td>\n",
       "      <td>1999</td>\n",
       "      <td>2011</td>\n",
       "      <td>2022</td>\n",
       "      <td>2037</td>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Years elapsed</td>\n",
       "      <td>200,000+</td>\n",
       "      <td>123</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0     Population         1     2     3     4     5     6     7     8     9  \\\n",
       "1           Year      1804  1927  1960  1974  1987  1999  2011  2022  2037   \n",
       "2  Years elapsed  200,000+   123    33    14    13    12    12    11    15   \n",
       "\n",
       "0    10  \n",
       "1  2057  \n",
       "2    20  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the table data\n",
    "df = pd.DataFrame(table_data)\n",
    "\n",
    "# Rename the columns based on the header row (assuming the first row is the header)\n",
    "df.columns = df.iloc[0]  # Set the first row as column headers\n",
    "df = df.drop(0)  # Drop the header row from the data\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Save the Data to a CSV File**: We can save the extracted table to a CSV file for further analysis or use.\n",
    "\n",
    "    - You can also save the DataFrame as a table in a database via `SQLAlchemy`, `SQLite`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table data has been saved to 'world_population.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('data/world_population.csv', index=False)\n",
    "\n",
    "print(\"Table data has been saved to 'world_population.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Dynamic Websites\n",
    "Some websites load content dynamically using JavaScript. In these cases, the content may not be immediately available in the HTML source code. To handle this, we can use **Selenium**, a browser automation tool that interacts with web pages like a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features of Selenium:\n",
    "- **JavaScript Rendering**: Waits for dynamic content to load before scraping.\n",
    "\n",
    "- **User Interaction Simulation**: Can mimic actions like clicks, form submissions, and scrolling.\n",
    "\n",
    "- **Multi-step Navigation**: Automates workflows that require user actions, such as logging into websites.\n",
    "\n",
    "### Pros of Selenium:\n",
    "- Works with multiple browsers (Chrome, Firefox, etc.).\n",
    "\n",
    "- Supports **headless mode** for environments without a graphical interface.\n",
    "\n",
    "### Limitations:\n",
    "- Resource-intensive and slower compared to static scrapers.\n",
    "\n",
    "- Running in environments like Codespaces can be challenging due to GUI limitations.\n",
    "\n",
    "### Alternatives:\n",
    "- **Playwright**: A robust alternative that also supports headless browsing and handles dynamic content.\n",
    "\n",
    "- **Splash**: A lightweight option for rendering JavaScript-heavy pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics of Web Scraping\n",
    "While web scraping is a powerful tool, it is essential to follow ethical guidelines and avoid scraping data without permission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Ethical Considerations:\n",
    "1. Check the `robots.txt` file: Many websites specify scraping permissions in their `robots.txt` file. Always check this file before scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robots.txt file fetched successfully!\n",
      "﻿# robots.txt for http://www.wikipedia.org/ and friends\n",
      "#\n",
      "# Please note: There are a lot of pages on this site, and there are\n",
      "# some misbehaved spiders out there that go _way_ too fast. If you're\n",
      "# irresponsible, your access to the site may be blocked.\n",
      "#\n",
      "\n",
      "# Observed spamming large amounts of https://en.wikipedia.org/?curid=NNNNNN\n",
      "# and ignoring 429 ratelimit responses, claims to respect robots:\n",
      "# http://mj12bot.com/\n",
      "User-agent: MJ12bot\n",
      "Disallow: /\n",
      "\n",
      "# advertising-related bots:\n",
      "User-agent: Mediapa\n"
     ]
    }
   ],
   "source": [
    "# URL of Wikipedia's robots.txt file\n",
    "robots_url = \"https://en.wikipedia.org/robots.txt\"\n",
    "\n",
    "# Send a GET request to fetch the robots.txt file\n",
    "response = requests.get(robots_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"robots.txt file fetched successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch robots.txt file: {response.status_code}\")\n",
    "\n",
    "# Print the contents of the robots.txt file\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `robots.txt` file for Wikipedia contains rules that control which sections of the website can be crawled or scraped. Let's break down some of the key parts of the file.\n",
    "\n",
    "- **User-agent: \\***: This means the rules apply to all web crawlers and scrapers.\n",
    "\n",
    "- **Disallow**: These are paths that crawlers are not allowed to access. For example, crawlers are instructed not to scrape URLs starting with `/wiki/Special`: or `/w/`.\n",
    "\n",
    "- **Allow**: The `/wiki/` path is allowed for crawling, meaning the general pages of Wikipedia can be accessed by crawlers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Avoid Overloading Servers**: Sending too many requests in a short period can overwhelm a server. Be sure to include delays between requests to avoid overloading the server.\n",
    "\n",
    "3. **Legal Implications**: Scraping copyrighted or sensitive data without permission can lead to legal consequences. Always ensure you are complying with the website’s terms of service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This lesson covers the full process of web scraping, from understanding HTTP and HTML to scraping static and dynamic websites. You should now have an understanding of how to:\n",
    "- Fetch and parse web pages.\n",
    "\n",
    "- Extract specific data elements.\n",
    "\n",
    "- Store data in structured formats like Pandas DataFrames.\n",
    "\n",
    "- Ethically and legally scrape websites, ensuring they respect the rules set by each site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [Selenium Web Automation Demo](https://www.youtube.com/watch?v=h8E_beaVYgs)\n",
    "\n",
    "- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "- [Requests Documentation](https://docs.python-requests.org/en/latest/)\n",
    "\n",
    "- [List of HTTP Status Codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
