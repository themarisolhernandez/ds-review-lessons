{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 17 Introduction to Simple Linear Regression "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn import metrics\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["## What is Linear Regression?\n","**Linear regression** is a statistical method used to model the relationship between a dependent variable (target) and one or more independent variables (features). It is widely used for prediction and forecasting in various fields such as economics, finance, and science.\n","\n","Assuming that there is a linear relationship between the independent variable(s) and the dependent variable and that this relationship can be represented by a straight line (or hyperplane), it aims to find the best-fitting linear equation that describes this relationship. \n","\n","### Applications:\n","- **Predicting Exam Scores:** Imagine you have data on students' study hours and their corresponding exam scores. You can use linear regression to predict a student's exam score based on the number of hours they studied.\n","\n","- **Forecasting House Prices:** Suppose you have data on house sizes (in square feet) and their selling prices. You can use linear regression to predict the selling price of a house based on its size.\n","\n","- **Estimating Gas Mileage:** If you have data on cars' engine sizes and their corresponding gas mileage, you can use linear regression to predict a car's gas mileage based on its engine size.\n","\n","## Theory Behind Linear Regression"]},{"cell_type":"markdown","metadata":{},"source":["Recall the equation for a straight line from your early math classes,\n","\n","$$ y = mx + b$$\n","\n","The equation represents a straight line where $m$ is the slope and $b$ is the y-intercept.\n","\n","Here's a Python code example using `matplotlib` to plot the line represented by the equation $ y = mx + b$ and allowing you to adjust the values of $m$ and $b$."]},{"cell_type":"code","execution_count":10,"metadata":{"metadata":{}},"outputs":[{"data":{"text/plain":["<Figure size 500x400 with 0 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6f65a741147447881fecf5b88950265","version_major":2,"version_minor":0},"text/plain":["interactive(children=(IntSlider(value=0, description='m', max=1), IntSlider(value=0, description='b', max=1), …"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<function __main__.plot_line(m, b)>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from ipywidgets import interact, FloatSlider, Checkbox\n","\n","# Function to plot the line\n","def plot_line(m, b):\n","    x_vals = np.linspace(0, 10, 100)\n","    y_vals = m * x_vals + b\n","    plt.figure(figsize=(5, 4))\n","    plt.plot(x_vals, y_vals, color='red', label=f'y = {m:.2f}x + {b:.2f}')\n","    plt.xlabel('X')\n","    plt.ylabel('y')\n","    plt.title('y = mx + b')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.xlim(0, 10)\n","    plt.ylim(-20, 20)\n","\n","# Define sliders for m and b\n","m_slider = FloatSlider(min=-10, max=10, step=1, value=0, description='Slope (m)')\n","b_slider = FloatSlider(min=-10, max=10, step=1, value=0, description='Intercept (b)')\n","\n","# Create interactive plot\n","interact(plot_line, m=m_slider, b=b_slider)"]},{"cell_type":"markdown","metadata":{},"source":["Now imagine we have data on house sizes (in square feet) and their selling prices. Below, we generate some synthetic data for house sizes and selling prices."]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Generate some random data for house sizes and selling prices\n","np.random.seed(0)\n","X = np.random.randint(1000, 3000, 50)  # House sizes in square feet\n","y = 50 * X + np.random.normal(0, 10000, 50)  # Selling prices\n","\n","# Create a DataFrame\n","df = pd.DataFrame({'House Size (sqft)': X, 'Selling Price': y})\n","\n","# Display the DataFrame\n","df"]},{"cell_type":"markdown","metadata":{},"source":["Now let's plot these data points in a scatter plot."]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Function to plot the data and multiple lines with different fits\n","def plot_data_and_lines(show_lines, show_legend):\n","    plt.figure(figsize=(9, 5))\n","    plt.scatter(X, y, color='blue', label='Data')\n","\n","    # Plot multiple lines with different fits\n","    if show_lines:\n","        for i in range(25, 100, 25):  # Generate 5 lines with different fits\n","            m = i\n","            b = np.mean(y) - m * np.mean(X)  # Calculate intercept\n","            x_vals = np.linspace(min(X), max(X), 100)\n","            y_vals = m * x_vals + b\n","            plt.plot(x_vals, y_vals, label=f'$\\hat{{y}} = {m:.2f}x + {b:.2f}$')\n","\n","    # Show legend if specified\n","    if show_legend:\n","        plt.legend()\n","\n","    plt.xlabel('House Size (sqft)\\n$(x)$')\n","    plt.ylabel('Selling Price\\n$(y)$')\n","    plt.title('House Size vs Selling Price')\n","    plt.grid(True)\n","    plt.show()\n","\n","# Checkbox widget to show/hide the lines\n","lines_checkbox = Checkbox(value=False, description='Show Lines')  # Default value set to False\n","\n","# Checkbox widget to show/hide the legend\n","legend_checkbox = Checkbox(value=False, description='Show Legend')  # Default value set to False\n","\n","# Create interactive plot with checkboxes\n","interact(plot_data_and_lines, show_lines=lines_checkbox, show_legend=legend_checkbox)"]},{"cell_type":"markdown","metadata":{},"source":["Let's imagine 3 data scientists are working with the same dataset. If each scientist draws a different line of fit, how do they decide which line is best?\n","\n","How can we find a simple linear equation that best represents the relationship between the dependent variable, *Selling Price*, and the independent variable, *House Size (sqft)*? In other words, how do we find the **line of best fit**?"]},{"cell_type":"markdown","metadata":{},"source":["### Line of Best Fit:\n","The line of best fit represents the linear relationship between the independent variable (predictor) and the dependent variable (response). In our case, the independent variable is *House Size (sqft)* and the dependent variable is *Selling Price*. \n","\n","This line is often determined through linear regression, which aims to minimize **the difference between the observed values and the values predicted by the line**.\n","\n","### What are Residuals?\n","Residuals, denoted as $ε$ (epsilon), are the differences between the observed values ($y$) and the values predicted by the model ($\\hat{y}$). In other words, they represent the error in the model's predictions. Mathematically, residuals can be expressed as,\n","\n","$$ε_{i} = y_{i} - \\hat{y}_{i}$$\n","$$~~~~~~~~~~~~~~~~~~= y_{i} - (mx_{i} + b)$$\n","\n","where:\n","- $ε_{i}$ is the error or residual for the $i$-th data point\n","\n","- $y_{i}$ is the observed (actual) value for the $i$-th data point\n","\n","- $\\hat{y}_{i}$ is the predicted value by the model for the $i$-th data point\n","\n","- $m$ represents the slope of the line in a linear regression model\n","\n","- $x_{i}$ represents the value of the independent variable for the $i$-th data point\n","\n","- $b$ represents the y-intercept of the line in a linear regression model\n","\n","A residual is a measure of how well a line fits an individual data point. Consider this simple data set with a line of fit drawn through it.\n","\n","<p align=\"center\">\n","  <img src=\"imgs/residual1.png\" alt=\"Alt text\" width=\"400\" height=\"400\">\n","</p>\n","\n","and notice how point **(2, 8)** is **<span style=\"color:green\">4</span>** units above the line:\n","\n","<p align=\"center\">\n","  <img src=\"imgs/residual2.png\" alt=\"Alt text\" width=\"400\" height=\"400\">\n","</p>\n","\n","This vertical distance is known as a **residual**. For data points above the line, the residual is positive, and for data points below the line, the residual is negative.\n","\n","For example, the residual for the point **(4, 3)** is **<span style=\"color:red\">-2</span>**.\n","\n","<p align=\"center\">\n","  <img src=\"imgs/residual3.png\" alt=\"Alt text\" width=\"400\" height=\"400\">\n","</p>\n","\n","The closer a data point's residual is to 0 the better the fit. In this case, the line fits the point (4, 3) better than (2, 8)."]},{"cell_type":"markdown","metadata":{},"source":["### Visualizing Residuals\n","We can further explore residuals by visualizing how they relate to our linear regression model for our housing dataset. Here, we can adjust the slope ($m$) and intercept ($b$) of the regression line and observe the corresponding residuals."]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Function to plot the data, line, and residuals\n","def plot_data_line_residuals(m, b, show_line, show_residuals):\n","    plt.figure(figsize=(9, 5))\n","    plt.scatter(X, y, color='blue', label='Data')\n","\n","    # Calculate predicted prices using the selected m and b\n","    y_hat = m * X + b\n","    \n","    # Plot the regression line if show_line is True\n","    if show_line:\n","        plt.plot(X, y_hat, color='red', label=f'$\\hat{{y}} = {m}x + {b}$')\n","\n","    # Plot dashed lines representing residuals if show_residuals is True\n","    if show_residuals:\n","        for i in range(len(X)):\n","            plt.plot([X[i], X[i]], [y[i], y_hat[i]], color='green', linestyle='--', linewidth=0.8)\n","\n","        # Add legend for residuals if not already added\n","        handles, labels = plt.gca().get_legend_handles_labels()\n","        if 'Residuals' not in labels:\n","            plt.plot([], [], color='green', linestyle='--', label='Residuals')\n","\n","    plt.xlabel('House Size (sqft)\\n$(x)$')\n","    plt.ylabel('Selling Price\\n$(y)$')\n","    plt.title('House Size vs Selling Price')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","# Define sliders for m and b\n","m_slider = FloatSlider(min=-100, max=100, step=1, value=0, description='Slope (m)')\n","b_slider = FloatSlider(min=-50000, max=50000, step=1000, value=0, description='Intercept (b)')\n","\n","# Checkbox widget to show/hide the line\n","line_checkbox = Checkbox(value=False, description='Show Line')  # Default value set to False\n","\n","# Checkbox widget to show/hide the residuals\n","residuals_checkbox = Checkbox(value=False, description='Show Residuals')  # Default value set to False\n","\n","# Create interactive plot with sliders and checkboxes\n","interact(plot_data_line_residuals, show_line=line_checkbox, show_residuals=residuals_checkbox, m=m_slider, b=b_slider)\n"]},{"cell_type":"markdown","metadata":{},"source":["So how do we know we've found the model parameters ($m$ and $b$) for the **line of best fit**?"]},{"cell_type":"markdown","metadata":{},"source":["### Determining Model Parameters for the Line of Best Fit\n","#### Method of Least Squares\n","In linear regression, the model parameters for the line of best fit are determined using the **method of least squares**. This method aims to <u>minimize</u> the sum of the squared differences between the observed values and the values predicted by the regression line.\n","\n","#### Mathematical Formulation\n","Given a set of $n$ data points $(x_{i}, y_{i})$, where $x_{i}$ represents the independent variable and $y_{i}$ represents the corresponding dependent variable, the line of best fit is represented by the equation:\n","\n","$$ \\hat{y}_{i} = mx_{i} + b$$\n","\n","where:\n","- $\\hat{y}_{i}$ is the predicted value by the model for the $i$-th data point\n","\n","- $m$ is the slope of the line (coefficient for the independent variable $x$)\n","\n","- $x_{i}$ represents the value of the independent variable for the $i$-th data point\n","\n","- $b$ is the y-intercept of the line\n","\n","The goal is to find the values of $m$ and $b$ that minimize the **sum of the squared errors**, denoted as $SSE$:\n","\n","$$SSE = \\sum_{i=1}^{n}ε_{i}^2 = \\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^2 = \\sum_{i=1}^{n}(y_{i} - (mx_{i} + b))^2$$\n","\n","where:\n","- $\\hat{y}_{i}$ is the predicted value of $y_{i}$ at the $i$-th data point\n","\n","With our same housing dataset, we can adjust the slope ($m$) and intercept ($b$) of the regression line and observe the corresponding sum of squared errors."]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Function to plot the data, line, residuals, and SSE\n","def plot_data_line_residuals_sse(m, b, show_line, show_residuals):\n","    plt.figure(figsize=(9, 5))\n","    plt.scatter(X, y, color='blue', label='Data')\n","\n","    # Calculate predicted prices using the selected m and b\n","    y_hat = m * X + b\n","    \n","    # Calculate residuals\n","    residuals = y - y_hat\n","    \n","    # Calculate sum of squared residuals if the line is shown\n","    if show_line:\n","        sum_squared_residuals = np.sum(residuals**2)\n","    else:\n","        sum_squared_residuals = None\n","    \n","    # Plot the regression line if show_line is True\n","    if show_line:\n","        plt.plot(X, y_hat, color='red', label=f'$\\hat{{y}} = {m}x + {b}$')\n","\n","    # Plot dashed lines representing residuals if show_residuals is True\n","    if show_residuals:\n","        for i in range(len(X)):\n","            plt.plot([X[i], X[i]], [y[i], y_hat[i]], color='green', linestyle='--', linewidth=0.8)\n","\n","        # Add legend for residuals if not already added\n","        handles, labels = plt.gca().get_legend_handles_labels()\n","        if 'Residuals' not in labels:\n","            plt.plot([], [], color='green', linestyle='--', label='Residuals')\n","\n","    plt.xlabel('House Size (sqft)\\n$(x)$')\n","    plt.ylabel('Selling Price\\n$(y)$')\n","    title = 'House Size vs Selling Price'\n","    if show_line:\n","        title += f'\\nSum of Squared Errors: {sum_squared_residuals:.2e}'\n","    plt.title(title)\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","# Define sliders for m and b\n","m_slider = FloatSlider(min=-100, max=100, step=1, value=0, description='Slope (m)')\n","b_slider = FloatSlider(min=-50000, max=50000, step=1000, value=0, description='Intercept (b)')\n","\n","# Checkbox widget to show/hide the line\n","line_checkbox = Checkbox(value=False, description='Show Line')  # Default value set to False\n","\n","# Checkbox widget to show/hide the residuals\n","residuals_checkbox = Checkbox(value=False, description='Show Residuals')  # Default value set to False\n","\n","# Create interactive plot with sliders and checkboxes\n","interact(plot_data_line_residuals_sse, show_line=line_checkbox, show_residuals=residuals_checkbox, m=m_slider, b=b_slider)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Introduction to Cost Function\n","Now that we've discussed the method of least squares and the goal of minimizing the sum of squared errors ($SSE$) to find the coefficients for the line of best fit, let's delve deeper into the concept of the **cost function**.\n","\n","In the realm of machine learning and optimization algorithms, a **cost function** serves as a critical component in evaluating the performance of a model. Also known as a loss function or objective function, it quantifies how well the model's predictions align with the actual observed values in the training dataset.\n","\n","##### Purpose of Cost Function\n","The primary purpose of a cost function is twofold:\n","\n","1. **Evaluation of Model Performance**: By assessing the extent of error or deviation between the predicted and actual values, the cost function provides insights into the efficacy of the model in capturing the underlying patterns and relationships within the data. A lower cost indicates better alignment between predictions and observations, signifying higher model accuracy.\n","\n","2. **Optimization**: Beyond evaluation, the cost function plays a pivotal role in the optimization process, guiding the iterative adjustment of model parameters to minimize prediction errors. Optimization algorithms, such as gradient descent, leverage the gradient (partial derivatives) of the cost function with respect to the model parameters to iteratively update the parameters and converge towards the optimal solution. More on this later...\n","\n","#### Cost Function: Sum of Squared Errors ($SSE$)\n","As we've seen earlier, the sum of squared errors ($SSE$) serves as a measure of the discrepancy between the observed values and the values predicted by our regression line. While $SSE$ is effective in quantifying the overall error, it has some limitations.\n","\n","#### Limitations of $SSE$\n","Although $SSE$ provides valuable insight into the model's performance, it does not account for the number of data points in the dataset. As a result, $SSE$ may vary significantly depending on the size of the dataset, making it challenging to compare models trained on different datasets directly.\n","\n","#### Introducing Mean Squared Error ($MSE$)\n","To address the limitations of $SSE$, we introduce the concept of **Mean Squared Error** ($MSE$). $MSE$ is obtained by dividing the $SSE$ by the number of data points, resulting in the average squared error per data point. Mathematically, it is written as:\n","\n","$$ MSE = \\frac{1}{n}SSE = \\frac{1}{n}\\sum_{i=1}^{n}ε_{i}^2 = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^2 = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - (mx_{i} + b))^2$$\n","\n","This normalization ensures that the cost function is independent of the dataset size, ensuring that the loss function is consistent across datasets of varying sizes. Additionally, $MSE$ provides a more intuitive measure of the model's performance, representing the average squared difference between the predicted and actual values.\n","\n","With our same housing dataset, we can adjust the slope ($m$) and intercept ($b$) of the regression line and observe the corresponding mean squared error ($MSE$)."]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Function to plot the data, line, residuals, and MSE\n","def plot_data_line_residuals_mse(m, b, show_line, show_residuals):\n","    plt.figure(figsize=(9, 5))\n","    plt.scatter(X, y, color='blue', label='Data')\n","\n","    # Calculate predicted prices using the selected m and b\n","    y_hat = m * X + b\n","    \n","    # Calculate residuals\n","    residuals = y - y_hat\n","    n = len(residuals)\n","    \n","    # Calculate mean sequared error if the line is shown\n","    if show_line:\n","        mean_squared_error = (1/n)*np.sum(residuals**2)\n","    else:\n","        mean_squared_error = None\n","    \n","    # Plot the regression line if show_line is True\n","    if show_line:\n","        plt.plot(X, y_hat, color='red', label=f'$\\hat{{y}} = {m}x + {b}$')\n","\n","    # Plot dashed lines representing residuals if show_residuals is True\n","    if show_residuals:\n","        for i in range(len(X)):\n","            plt.plot([X[i], X[i]], [y[i], y_hat[i]], color='green', linestyle='--', linewidth=0.8)\n","\n","        # Add legend for residuals if not already added\n","        handles, labels = plt.gca().get_legend_handles_labels()\n","        if 'Residuals' not in labels:\n","            plt.plot([], [], color='green', linestyle='--', label='Residuals')\n","\n","    plt.xlabel('House Size (sqft)\\n$x$')\n","    plt.ylabel('Selling Price\\n$y$')\n","    title = 'House Size vs Selling Price'\n","    if show_line:\n","        title += f'\\nMean Squared Error: {mean_squared_error:.2e}'\n","    plt.title(title)\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","# Define sliders for m and b\n","m_slider = FloatSlider(min=-100, max=100, step=1, value=0, description='Slope (m)')\n","b_slider = FloatSlider(min=-50000, max=50000, step=1000, value=0, description='Intercept (b)')\n","\n","# Checkbox widget to show/hide the line\n","line_checkbox = Checkbox(value=False, description='Show Line')  # Default value set to False\n","\n","# Checkbox widget to show/hide the residuals\n","residuals_checkbox = Checkbox(value=False, description='Show Residuals')  # Default value set to False\n","\n","# Create interactive plot with sliders and checkboxes\n","interact(plot_data_line_residuals_mse, show_line=line_checkbox, show_residuals=residuals_checkbox, m=m_slider, b=b_slider)"]},{"cell_type":"markdown","metadata":{},"source":["#### Gradient Descent\n","Now that we've established $MSE$ as our preferred cost function, let's explore how we can optimize our linear regression model using **gradient descent**. Gradient descent is an iterative optimization algorithm that aims to <u>minimize</u> the cost function ($MSE$) by adjusting the model parameters (slope and intercept).\n","\n","##### Mathematics Behind Gradient Descent \n","Up to this point, we've been using $\\hat{y}_{i}$ to represent the predicted value for the $i$-th training example,\n","\n","$$ \\hat{y}_{i} = mx_{i} + b$$\n","\n","where:\n","- $\\hat{y}_{i}$ is the predicted value for the $i$-th data point\n","\n","- $m$ is the slope of the line (coefficient for the independent variable $x$)\n","\n","- $x_{i}$ represents the value of the independent variable for the $i$-th data point\n","\n","- $b$ is the y-intercept of the line\n","\n","We can also express the linear regression with the following notation,\n","\n","$$ \\hat{y}_{i} = w_{0} + w_{1}x_{i}$$\n","\n","where:\n","\n","- $\\hat{y}_{i}$ is the predicted value for the $i$-th data point\n","\n","- $w_{0}$ corresponds to the y-intercept ($b$)\n","\n","- $w_{1}$ corresponds to the slope ($m$)\n","\n","- $x_{i}$ represents the value of the independent variable for the $i$-th data point\n","\n","To minimize the cost function ($MSE$), the model needs to find the best value of $w_{0}$ and $w_{1}$. We will find the optimal values for $w_{0}$ and $w_{1}$ using gradient descent in a step-by-step process.\n","\n","##### Step-by-Step Process of Gradient Descent\n","1. **Initialization**: We initialize the values of $w_{0}$ and $w_{1}$ to some random values or zeros. These values represent the parameters (y-intercept and slope) of the linear regression model.\n","\n","2. **Define the Cost Function:** We define the Mean Squared Error ($MSE$) as our cost function. The $MSE$ represents the average squared difference between the predicted and actual values over all data points\n","\n","$$ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^2$$\n","\n","We can actually scale our cost function using a factor of $\\frac{1}{2n}$ so that we get a cost function that looks like,\n","\n","$$ J(w) = \\frac{1}{2n}\\sum_{i=1}^{n}(\\hat{y}_{i} - y_{i})^2$$\n","\n","The factor $\\frac{1}{2}$ is often included for mathematical convenience as it simplifies the derivative computation in the next step. \n","\n","The subtraction $\\hat{y}_{i} - y_{i}$ is just a rearrangement; it serves the same purpose of quantifying the discrepancy between predicted and actual values, ultimately resulting in the same optimization goal of minimizing the cost function. For example, $(3 - 1)^2 = 2^2 = 4$ which is the same as  $(1 - 3)^2 = (-2)^2 = 4$\n","\n","3. **Compute the Gradient:** Compute the gradient of the cost function with respect to each parameter. It involves taking the partial differentiation of the cost function with respect to each of the parameters.\n","\n","Partial derivative with respect to $w_0$ simplifies to,\n","\n","$$\\frac{\\partial J(w)}{\\partial w_0} = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_{i} - y_{i})$$ \n","\n","Partial derivative with respect to $w_1$ simplifies to,\n","\n","$$\\frac{\\partial J(w)}{\\partial w_1} = \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_{i} - y_{i}) \\cdot x_{i}$$ \n","\n","4. **Set the Learning Rate:** We choose a learning rate, denoted as $\\alpha$, which determines the size of the steps we take in the direction of the gradient.\n","\n","5. **Update the Parameters:** Using the gradient and the learning rate, we update the parameters iteratively. The update rule for each parameter is:\n","\n","$$w_{0} = w_{0} - \\alpha \\cdot \\frac{\\partial J(w)}{\\partial w_0}$$\n","\n","$$w_{1} = w_{1} - \\alpha \\cdot \\frac{\\partial J(w)}{\\partial w_1}$$\n","\n","The generalized update rule for each model parameter is\n","\n","$$\\text{new coefficient} = \\text{old coefficient} - (\\text{learning rate} * \\text{gradient})$$\n","\n","We apply this update rule to each parameter, moving them in the direction that reduces the cost function.\n","\n","6. **Repeat Until Convergence:** We repeat steps 3 through 5 until the cost function converges to a minimum. Convergence is typically checked by monitoring the change in the cost function or after a predetermined number of iterations.\n","\n","7. **Final Parameters:** Once the algorithm converges, the final parameters $w_0$ and $w_1$ represent the best-fit line for our linear regression model.\n","\n","Lets try to visualize this process by plotting our cost function ($MSE$) against one of our model parameter's, specifically $w_1$ which represents the slope of our regression line. $w_1$ also represents the weight or coefficient of the parameter *House Size (sqft)*."]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Define the cost function\n","def cost_function(w0, w1, x, y):\n","    y_hat = w0 + w1 * x\n","    errors = y_hat - y\n","    n = len(x)\n","    cost = (1 / (2 * n)) * np.sum(errors ** 2)\n","    return cost\n","\n","# Derivative of the cost function with respect to theta1 (slope)\n","def w1_partial_derivative(w0, w1, x, y):\n","    y_hat = w0 + w1 * x\n","    errors = y_hat - y\n","    n = len(x)\n","    derivative = (1 / n) * np.sum(errors * x) \n","    return derivative\n","\n","# Interactive function to plot the cost function against a specified weight\n","def plot_cost_function(weight_value, show_tangent_line):\n","    # Create a range of weight values for plotting\n","    weights = np.linspace(-1000, 1000, 100)\n","    \n","    # Calculate the cost for each weight value\n","    costs = [cost_function(0, w, X, y) for w in weights]\n","    \n","    # Plot the cost function\n","    plt.figure(figsize=(9, 5))\n","    plt.plot(weights, costs, label='Cost Function')\n","    \n","    # Plot the specified weight value as a green point\n","    plt.plot(weight_value, cost_function(0, weight_value, X, y), 'o', markersize=8, color='black')\n","    \n","    if show_tangent_line:\n","        # Calculate the slope of the tangent line (derivative of the cost function)\n","        slope = w1_partial_derivative(0, weight_value, X, y)\n","        \n","        # Plot the tangent line\n","        tangent_x = np.linspace(weight_value - 300, weight_value + 300, 100)\n","        tangent_y = slope * (tangent_x - weight_value) + cost_function(0, weight_value, X, y)\n","        plt.plot(tangent_x, tangent_y, 'r--', label='Tangent Line')\n","    \n","    plt.xlabel('$w_1$')\n","    plt.ylabel('MSE')\n","    plt.title('Cost Function (MSE) vs $w_1$')\n","    plt.grid(True)\n","    plt.legend()\n","    plt.show()\n","\n","# Define sliders for weight value and tangent line visibility\n","weight_slider = FloatSlider(min=-1000, max=1000, step=10, value=-700, description='w_1')\n","tangent_line_checkbox = Checkbox(value=False, description='Show Tangent Line')\n","\n","# Create interactive plot\n","interact(plot_cost_function, weight_value=weight_slider, show_tangent_line=tangent_line_checkbox)"]},{"cell_type":"markdown","metadata":{},"source":["In the plot above, The cost function quantifies how well our model fits the training data for different values of $w_1$.\n","\n","Now, let's focus on a specific point on the cost function curve. At that point, the tangent line touches the curve, indicating the slope of the curve at that precise location. The gradient of the cost function at this point gives us the slope of this tangent line. \n","\n","This gradient essentially tells us how the cost function will change and in what direction if we adjust $w_1$ slightly.\n","\n","**If the slope of the tangent line is +positive**: $~~w_j = w_j - \\alpha~\\cdot~$ (+ve value). Hence the value of $w_j$ decreases.\n","\n","<p align=\"center\">\n","  <img src=\"imgs/gradient_descent1.png\" alt=\"Alt text\" width=\"400\" height=\"400\">\n","</p>\n","\n","**If the slope of the tangent line is -negative**: $~~w_j = w_j - \\alpha~\\cdot~$ (-ve value). Hence the value of $w_j$ increases.\n","\n","<p align=\"center\">\n","  <img src=\"imgs/gradient_descent2.png\" alt=\"Alt text\" width=\"400\" height=\"400\">\n","</p>"]},{"cell_type":"markdown","metadata":{},"source":["Again, let's visualize the cost function ($MSE$) against the parameter $w_1$ which represents the slope of the regression line. As a reminder, the goal of gradient descent is to find the optimal value of $w_1$ that minimizes the cost function."]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Define the cost function\n","def cost_function(w0, w1, x, y):\n","    predictions = w0 + w1 * x\n","    errors = predictions - y\n","    n = len(x)\n","    cost = (1 / (2 * n)) * np.sum(errors ** 2)\n","    return cost\n","\n","# Compute the gradient of the cost function\n","def compute_gradient(w0, w1, x, y):\n","    predictions = w0 + w1 * x\n","    errors = predictions - y\n","    n = len(x)\n","    gradient_w0 = (1 / n) * np.sum(errors)\n","    gradient_w1 = (1 / n) * np.sum(errors * x)\n","    return gradient_w0, gradient_w1\n","\n","# Interactive function to plot the cost function and the steps to the minimum\n","def plot_steps(w1, show_steps):\n","    w0 = 0\n","    alpha = 0.0000001 \n","    num_iterations = 20     \n","\n","    # Create a range of weight values for plotting\n","    weights = np.linspace(-1000, 1000, 100)\n","    \n","    # Calculate the cost for each weight value\n","    costs = [cost_function(0, w, X, y) for w in weights]\n","    \n","    # Plot the cost function\n","    plt.figure(figsize=(9, 5))\n","    plt.plot(weights, costs, label='Cost Function')\n","    \n","    # Plot the starting point\n","    cost = cost_function(w0, w1, X, y)\n","    plt.plot(w1, cost, 'o', markersize=8, color=\"black\")\n","    # print(f\"Iteration 0: w_0 = {w0}, w_1 = {w1}, Cost = {cost}, Alpha: {alpha}\")\n","\n","    if show_steps:\n","        if w1 >= 52.74:\n","            rad = -0.3\n","        else:\n","            rad = 0.3\n","\n","        # Perform gradient descent\n","        for i in range(num_iterations):\n","            # Compute the gradient\n","            gradient_w0, gradient_w1 = compute_gradient(w0, w1, X, y)\n","            \n","            # Update the parameters\n","            prev_w0, prev_w1 = w0, w1\n","            w0 -= alpha * gradient_w0\n","            w1 -= alpha * gradient_w1\n","            \n","            # Compute the cost\n","            cost = cost_function(w0, w1, X, y)\n","            \n","            # Display the updated parameters and cost\n","            plt.plot(w1, cost, 'o', markersize=8, color=\"black\")\n","            \n","            # Add arrow indicating the direction of movement\n","            plt.annotate('', xy=(w1, cost), xytext=(prev_w1, cost_function(prev_w0, prev_w1, X, y)),\n","                         arrowprops=dict(arrowstyle='->', connectionstyle=f'arc3,rad={rad}', lw=1.5, color=\"red\"))\n","            # print(f\"Iteration {i + 1}: w_0 = {w0}, w_1 = {w1}, Cost = {cost}, Alpha: {alpha}\")\n","        \n","    \n","    plt.xlabel('$w_1$')\n","    plt.ylabel('MSE')\n","    plt.title('Cost Function (MSE) vs $w_1$')\n","    plt.grid(True)\n","    plt.legend()\n","    plt.show()\n","\n","\n","# Define sliders for initial weight and learning rate, and a checkbox for showing steps/path\n","w1_slider = FloatSlider(min=-1000, max=1000, step=10, value=-700, description='w_1')\n","steps_checkbox = Checkbox(value=False, description='Show Steps')\n","\n","# Create interactive plot\n","interact(plot_steps, w1=w1_slider, show_steps=steps_checkbox)"]},{"cell_type":"markdown","metadata":{},"source":["In the plot above, we can set the initial point which corresponds to to the initial chosen value of $w_1$. \n","\n","If the \"Show Steps\" checkbox is selected, the red arrows illustrate the iterative steps taken by the gradient descent algorithm to minimize the cost function. At each step, the algorithm computes the gradient of the cost function with respect to $w_1$ and updates $w_1$ (and $w_0$) accordingly. \n","\n","These updates happen iteratively until the cost function converges to a minimum or after a set number of iterations."]},{"cell_type":"markdown","metadata":{},"source":["#### How To Choose Learning Rate \n","The choice of the learning rate, $\\alpha$, is very important as it ensures that Gradient Descent converges in a reasonable time.\n","\n","If we choose **$\\alpha$ to be very large**, Gradient Descent can overshoot the minimum. It may fail to converge or even diverge.\n","\n","<p align=\"center\">\n","  <img src=\"imgs/alpha1.png\" alt=\"Alt text\" width=\"300\" height=\"300\">\n","</p>\n","\n","If we choose **$\\alpha$ to be very small**, Gradient Descent will take small steps to reach local minima and will take a longer time to reach minima. \n","\n","<p align=\"center\">\n","  <img src=\"imgs/alpha2.png\" alt=\"Alt text\" width=\"300\" height=\"300\">\n","</p>"]},{"cell_type":"markdown","metadata":{},"source":["### Linear Regression with `scikit-learn `\n","While implementing gradient descent from scratch offers deep insights into optimization techniques, we can leverage existing libraries like `scikit-learn` to streamline the process. `scikit-learn`  encapsulates complex optimization algorithms, such as gradient descent variants, within its user-friendly interface, allowing us to focus on model development rather than algorithmic intricacies.\n","\n","Here, we instantiate a Linear Regression model object and then train it using the fit method with our dataset. Behind the scenes, `scikit-learn` employs efficient optimization algorithms, including variants of gradient descent, to find the optimal parameters that minimize the difference between predicted and actual target values."]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Perform train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Reshape X for fitting\n","X_train = X_train.reshape(-1, 1)\n","X_test = X_test.reshape(-1, 1)\n","\n","# Fit the model on the training data\n","model = LinearRegression()\n","model.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["### Interpreting the Final Model Parameters\n","Having trained our Linear Regression model using `scikit-learn`, we've successfully optimized its parameters to best fit the data. Let's examine the final model parameters:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Intercept: {model.intercept_}\")\n","print(f\"Coefficients: {model.coef_}\")"]},{"cell_type":"markdown","metadata":{},"source":["With these values, we can write the equation of our final regression model,\n","\n","$$\\hat{y} = w_{0} + w_{1}x = -6744.28 + 52.52x$$\n","\n","where:\n","\n","- $\\hat{y}$ is the predicted (*Selling Price*) value \n","\n","- $w_{0}$ corresponds to the y-intercept ($b$)\n","\n","- $w_{1}$ corresponds to the slope ($m$)\n","\n","- $x$ represents the value of the independent variable (*House Size (sqft)*)\n","\n","The **intercept** represents the model's prediction when all features are zero. It accounts for the baseline value of the target variable, independent of the input features.\n","\n","On the other hand, the **coefficients** (also known as weights or slopes) quantify the impact of each feature on the target variable. The coefficient represents the change in the target variable for a one-unit change in the input feature, while holding all other features constant.\n","\n","For example, we can say that we expect the price to increase by $52.52 for every additional square foot of house size, with all other factors held constant.\n","\n","Let's plot our final regression model:"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Making predictions on entire dataset\n","y_hat = model.intercept_ + model.coef_[0]*X\n","# y_hat = model.predict(X)\n","\n","# Plotting the data points\n","plt.scatter(X_train, y_train, color='black', label='Training Set')\n","plt.scatter(X_test, y_test, color='blue', label='Test Set')\n","\n","# Plotting the regression line\n","plt.plot(X, y_hat, color='red', label=f'$\\hat{{y}} = {round(model.intercept_, 2)} + {round(model.coef_[0], 2)}x$ ')\n","\n","# Adding labels and title\n","plt.xlabel('House Size (sqft)\\n$(x)$')\n","plt.ylabel('Selling Price\\n$(y)$')\n","plt.title('Linear Regression Model')\n","plt.legend()\n","\n","# Displaying the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Model Performance Metrics\n","After building a regression model, it's crucial to assess its performance using appropriate metrics. These metrics help quantify how well the model fits the data and how accurate its predictions are.\n","\n","#### Mean Absolute Error (MAE)\n","\n","Mean Absolute Error (MAE) measures the average absolute difference between the predicted values and the actual values. It provides a straightforward interpretation of the model's predictive accuracy.\n","\n","Mathematically, MAE is calculated as:\n","\n","$$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n","\n","where:\n","- $ n $ is the number of samples\n","- $ y_i $ is the actual target value for the $ i $-th sample\n","- $ \\hat{y}_i $ is the predicted target value for the $ i $-th sample\n","\n","#### Mean Squared Error (MSE)\n","\n","Mean Squared Error (MSE) measures the average squared difference between the predicted values and the actual values. It penalizes larger errors more heavily than smaller ones.\n","\n","Mathematically, MSE is calculated as:\n","\n","$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n","\n","#### Root Mean Squared Error (RMSE)\n","\n","Root Mean Squared Error (RMSE) is the square root of the Mean Squared Error. It is often preferred over MSE because it is in the same units as the target variable, making it more interpretable.\n","\n","Mathematically, RMSE is calculated as:\n","\n","$$ RMSE = \\sqrt{MSE} $$\n","\n","#### R-squared ($ R^2 $) Score\n","\n","R-squared ($ R^2 $) score, also known as the coefficient of determination, measures the proportion of the variance in the target variable that is predictable from the independent variables. It provides an indication of the goodness of fit of the model.\n","\n","The $ R^2 $ score ranges from 0 to 1, where:\n","- 0 indicates that the model does not explain any variability in the target variable\n","- 1 indicates that the model perfectly explains the variability in the target variable\n","\n","The $ R^2 $ score is calculated as:\n","\n","$$ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\n","\n","where:\n","- $ \\bar{y} $ is the mean of the actual target values\n","\n","When interpreting the $ R^2 $ score, it's essential to compare it with the $ R^2 $ score of a baseline model (e.g., a model that always predicts the mean of the target variable). A higher $ R^2 $ score indicates that the model performs better than the baseline model.\n","\n","#### Implementing Model Performance Metrics with `scikit-learn`\n","\n","To calculate these performance metrics using `scikit-learn`, we can use the appropriate functions provided by the `metrics` module. For example:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make predictions on the test data\n","y_hat = model.predict(X_test)\n","\n","# Create a DataFrame for X_test and y_test\n","df_test = pd.DataFrame({\n","    'House Size (sqft)': X_test.flatten(), \n","    'Selling Price': y_test, \n","    'Predictions': y_hat},\n","    )\n","df_test"]},{"cell_type":"markdown","metadata":{},"source":["When evaluating a linear regression model, it's important to assess its performance on both the training set and the test set to evaluate for underfitting/overfitting. For just this lesson, we are evaluating model metrics for just the test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate MAE\n","mae = metrics.mean_absolute_error(y_test, y_hat)\n","\n","# Calculate MSE\n","# mse = metrics.mean_squared_error(y_test, y_hat)\n","\n","# Calculate RMSE\n","# rmse = np.sqrt(mse)\n","\n","# Calculate R-squared score\n","r_squared = metrics.r2_score(y_test, y_hat)\n","\n","print(f\"Mean Absolute Error (MAE): {mae}\")\n","# print(f\"Mean Squared Error (MSE): {mse}\")\n","# print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","print(f\"R-squared Score (R^2): {r_squared}\")"]},{"cell_type":"markdown","metadata":{},"source":["With the calculated RMSE, we can tell our stakeholders that our model's predictions are off by ~$8,594.24 on average."]},{"cell_type":"markdown","metadata":{},"source":["### Making Predictions\n","We can use our linear regression model to make predictions. Lets say that some developers built a 2,000 sqft house and they need help settling on a selling price. Given the size, we can use our model to predict the selling price."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# X_new is your new data for which you want to make predictions\n","X_new = np.array(2000).reshape(1, -1)\n","\n","# Make predictions on new data\n","predicted_selling_price = model.predict(X_new)\n","\n","# Print the predictions\n","print(f\"We can advise the developers to list the house for: ${round(predicted_selling_price[0], 2)}\")"]},{"cell_type":"markdown","metadata":{},"source":["We made the prediction using the `scikit-learn` library. Let's calculate by hand to confirm. Remember our final model,\n","$$\\hat{y} = w_{0} + w_{1}x = -6744.28 + 52.52x$$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"w0 = {model.intercept_}\")\n","print(f\"w1 = {model.coef_[0]}\")\n","print(f\"x = 2000\")\n","print(f\"y = w0 + w1*x = {round(model.intercept_, 2)} + {round(model.coef_[0], 2)}*2000 = {round(model.intercept_ + model.coef_[0] * 2000, 2)}\")"]},{"cell_type":"markdown","metadata":{},"source":["We can also see where this prediction lies in our regression line,"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Making predictions on entire dataset\n","y_hat = model.intercept_ + model.coef_[0]*X\n","# y_hat = model.predict(X)\n","\n","# Plotting the data points\n","plt.scatter(X, y, color='black', label='Data Points')\n","\n","# Plotting the regression line\n","plt.plot(X, y_hat, color='red', label=f'$\\hat{{y}} = {round(model.intercept_, 2)} + {round(model.coef_[0], 2)}x$ ')\n","\n","# Plotting the predicted value\n","plt.scatter(X_new, predicted_selling_price, color='green', marker='*', s=200, zorder=5, label='Predicted Value')\n","\n","# Adding labels and title\n","plt.xlabel('House Size (sqft)\\n$(x)$')\n","plt.ylabel('Selling Price\\n$(y)$')\n","plt.title('Linear Regression Model')\n","plt.legend()\n","\n","# Displaying the plot\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":2}
